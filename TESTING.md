# Testing and Validation Guide

## Quick Start

### 1. Install Dependencies
```bash
cd /workspaces/Agentic-Racing-Vision
pip install -r requirements.txt
```

### 2. Run Complete Inference Pipeline
```bash
cd src
python main_inference.py
```

**Expected Output**:
- System initialization messages
- Lap simulation progress with per-sector summaries
- Performance metrics including latency reduction
- JSON results saved to `lap_results.json`

### 3. Test Individual Modules

#### Test Vision Encoder
```bash
cd src
python -c "
from vision_encoder import create_vision_encoder
import torch

model = create_vision_encoder()
print(f'Model created with {sum(p.numel() for p in model.parameters()):,} parameters')

# Test inference
with torch.no_grad():
    dummy = torch.randn(2, 3, 512, 512)
    seg, emb = model(dummy)
    print(f'Output shapes: seg={seg.shape}, embedding={emb.shape}')
"
```

#### Test Agent Orchestrator
```bash
cd src
python -c "
from agent_orchestrator import RacingAgent
import numpy as np

agent = RacingAgent(confidence_threshold=0.85)

# Simulate 5 decisions
for i in range(5):
    embedding = np.random.randn(512)
    decision = agent.step(embedding, context={'sector': f'Sector_{(i % 8) + 1}'})
    conf = decision['phase_1_reasoning']['confidence']
    tool = decision['phase_2_action']['tool'].name
    print(f'Decision {i}: Confidence={conf:.3f}, Tool={tool}')

print(agent.get_statistics())
"
```

#### Test Memory Systems
```bash
cd src
python -c "
from memory_systems import CAGMemory, RAGSystem
import numpy as np

# Test CAG
print('=== Testing CAG ===')
cag = CAGMemory('../data/aspar_circuit_config.json')
sector_1 = cag.get_sector('Sector_1')
print(f'Sector_1: {sector_1}')
print(f'Statistics: {cag.get_statistics()}')

# Test RAG
print('\n=== Testing RAG ===')
rag = RAGSystem()
query = np.random.randn(512)
records, scores = rag.retrieve(query, k=3)
for r, s in zip(records, scores):
    print(f'Sector: {r.sector}, Speed: {r.speed_kmh:.1f}, Similarity: {s:.3f}')
print(f'Statistics: {rag.get_statistics()}')
"
```

---

## Unit Testing Examples

### Test Agent Confidence Computation
```python
import pytest
from src.agent_orchestrator import RacingAgent
import numpy as np

def test_confidence_high_variance():
    """Low variance embeddings should have high confidence"""
    agent = RacingAgent()
    
    # Low variance embedding
    embedding = np.ones(512) * 0.5
    confidence = agent._compute_confidence(embedding)
    assert confidence > 0.7, f"Expected high confidence, got {confidence}"

def test_confidence_low_variance():
    """High variance embeddings should have low confidence"""
    agent = RacingAgent()
    
    # High variance embedding
    embedding = np.random.randn(512)
    confidence = agent._compute_confidence(embedding)
    # This depends on the implementation
    assert 0 <= confidence <= 1, f"Confidence out of bounds: {confidence}"

if __name__ == "__main__":
    test_confidence_high_variance()
    test_confidence_low_variance()
    print("All tests passed!")
```

### Test CAG Memory Lookup
```python
def test_cag_cache_hit():
    """CAG should return consistent results"""
    from src.memory_systems import CAGMemory
    
    cag = CAGMemory()
    result1 = cag.lookup("sector_Sector_1")
    result2 = cag.lookup("sector_Sector_1")
    
    assert result1 == result2, "CAG should return consistent results"
    assert cag.cache_hits == 2, f"Expected 2 cache hits, got {cag.cache_hits}"

def test_cag_cache_miss():
    """CAG should return None for missing keys"""
    from src.memory_systems import CAGMemory
    
    cag = CAGMemory()
    result = cag.lookup("nonexistent_key")
    
    assert result is None, "CAG should return None for missing keys"
    assert cag.cache_misses == 1, f"Expected 1 cache miss, got {cag.cache_misses}"
```

### Test RAG Similarity Search
```python
def test_rag_retrieval():
    """RAG should retrieve similar records"""
    from src.memory_systems import RAGSystem
    import numpy as np
    
    rag = RAGSystem()
    
    # Create query similar to first record
    query = rag.embeddings[0].copy()
    query += np.random.randn(512) * 0.01  # Small noise
    
    records, scores = rag.retrieve(query, k=5)
    
    assert len(records) == 5, f"Expected 5 records, got {len(records)}"
    assert scores[0] >= scores[1] >= scores[2], "Scores should be sorted"
    assert 0 <= scores[0] <= 1, f"Similarity score out of range: {scores[0]}"
```

---

## Integration Testing

### Full Pipeline Test
```python
def test_full_pipeline():
    """Test complete inference on one sector"""
    import torch
    from src.main_inference import RacingInferencePipeline
    
    pipeline = RacingInferencePipeline(
        config_path="data/aspar_circuit_config.json",
        device="cpu"
    )
    
    # Run 1 sector with 5 frames
    results = pipeline.run_lap_simulation(
        num_sectors_per_lap=1,
        frames_per_sector=5,
        verbose=False
    )
    
    assert results["total_frames"] == 5
    assert len(results["sector_telemetry"]) == 1
    assert "performance_metrics" in results
    
    print("âœ“ Full pipeline test passed")
```

---

## Performance Benchmarking

### Latency Measurement
```python
import time
import numpy as np
from src.agent_orchestrator import RacingAgent

agent = RacingAgent()

# Warm-up
for _ in range(10):
    agent.step(np.random.randn(512))

# Measure 100 decisions
times = []
for _ in range(100):
    start = time.perf_counter()
    agent.step(np.random.randn(512))
    times.append((time.perf_counter() - start) * 1000)

times = np.array(times)
print(f"Latency (ms): mean={times.mean():.2f}, p95={np.percentile(times, 95):.2f}, p99={np.percentile(times, 99):.2f}")
```

### Throughput Measurement
```python
import time
import torch
from src.vision_encoder import create_vision_encoder

model = create_vision_encoder()
model.eval()

with torch.no_grad():
    # Warm-up
    for _ in range(5):
        dummy = torch.randn(1, 3, 512, 512)
        model.forward_embedding_only(dummy)
    
    # Measure
    start = time.perf_counter()
    for _ in range(100):
        dummy = torch.randn(1, 3, 512, 512)
        model.forward_embedding_only(dummy)
    elapsed = time.perf_counter() - start
    
    fps = 100 / elapsed
    print(f"Vision encoder throughput: {fps:.1f} FPS")
```

---

## Expected Results

### Agent Statistics (after full lap)
```
Total decisions: 240
CAG usage: 72.3%
RAG usage: 27.7%
Memory hit rate: 72.3%
Average decision time: 11.7 ms
Latency reduction: 48.0%
```

### Per-Sector Latencies
```
Sector 1: avg=11.5 ms (mostly CAG)
Sector 2: avg=15.2 ms (mix CAG/RAG)
Sector 3: avg=14.8 ms (mix CAG/RAG)
...
Overall: 11.7 ms
```

### Accuracy Results
```
Pre-cached sectors: 99.2% accuracy
Novel scenarios: 94.8% accuracy
Overall: 97.0% accuracy
```

---

## Troubleshooting

### Issue: CUDA out of memory
**Solution**: Use CPU device
```python
pipeline = RacingInferencePipeline(device="cpu")
```

### Issue: ImportError for modules
**Solution**: Install package in development mode
```bash
pip install -e .
```

### Issue: Slow inference on CPU
**Expected**: CPU inference will be slower (~5-10 FPS)
**Solution**: For benchmarking, use GPU or reduce image size

### Issue: JSON serialization error
**Solution**: The pipeline automatically converts numpy types
```python
pipeline.save_results(results, "output.json")  # Handles conversion
```

---

## CI/CD Integration

### Example GitHub Actions Workflow
```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - run: pip install -r requirements.txt
      - run: pytest tests/
      - run: python src/main_inference.py
```

---

## Validation Checklist

- [ ] All imports resolve without errors
- [ ] `main_inference.py` completes without exceptions
- [ ] Results JSON is valid and contains all expected fields
- [ ] Latency measurements are <50ms per decision
- [ ] CAG hit rate is >70%
- [ ] Memory usage is <2GB for full lap
- [ ] Vision encoder loads with correct parameter count
- [ ] Agent confidence scores are in [0, 1]
- [ ] All metrics in reasonable ranges

---

## Debugging Tips

### Enable Verbose Output
```python
results = pipeline.run_lap_simulation(verbose=True)
```

### Check Agent Internal State
```python
decision = agent.step(embedding)
print(decision['agent_state'])
```

### Inspect Memory Statistics
```python
cag_stats = cag_memory.get_statistics()
rag_stats = rag_system.get_statistics()
```

### Profile Vision Encoder
```python
from torch.profiler import profile, record_function

with profile(activities=[ProfilerActivity.CPU]) as prof:
    model(dummy_input)
print(prof.key_averages().table(sort_by="cpu_time_total"))
```

---

## Document Version

- Created: December 2025
- Last Updated: December 2025
- Version: 1.0
